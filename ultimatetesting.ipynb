{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for Barcelona (2025-03-22 to 2025-03-28)\n",
      " Error during search and scrape: Message: Element <input id=\":rh:\" class=\"eb46370fe1\" name=\"ss\"> is not clickable at point (409,365) because another element <div class=\"eb33ef7c47\"> obscures it\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:193:5\n",
      "ElementClickInterceptedError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:337:5\n",
      "webdriverClickElement@chrome://remote/content/marionette/interaction.sys.mjs:177:11\n",
      "interaction.clickElement@chrome://remote/content/marionette/interaction.sys.mjs:136:11\n",
      "clickElement@chrome://remote/content/marionette/actors/MarionetteCommandsChild.sys.mjs:344:29\n",
      "receiveMessage@chrome://remote/content/marionette/actors/MarionetteCommandsChild.sys.mjs:220:31\n",
      "\n",
      " Closing browser for Barcelona\n",
      "Searching for Madrid (2025-03-22 to 2025-03-28)\n",
      "Searching for Madrid (2025-03-01 to 2025-03-07)\n",
      " Error during search and scrape: Message: Element <input id=\":rh:\" class=\"eb46370fe1\" name=\"ss\"> is not clickable at point (409,365) because another element <div class=\"eb33ef7c47\"> obscures it\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:193:5\n",
      "ElementClickInterceptedError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:337:5\n",
      "webdriverClickElement@chrome://remote/content/marionette/interaction.sys.mjs:177:11\n",
      "interaction.clickElement@chrome://remote/content/marionette/interaction.sys.mjs:136:11\n",
      "clickElement@chrome://remote/content/marionette/actors/MarionetteCommandsChild.sys.mjs:344:29\n",
      "receiveMessage@chrome://remote/content/marionette/actors/MarionetteCommandsChild.sys.mjs:220:31\n",
      "\n",
      " Closing browser for Madrid\n",
      "Searching for Barcelona (2025-03-01 to 2025-03-07)\n",
      "Scrolling and clicking 'See More' for Madrid...\n",
      "Scrolling and clicking 'See More' for Barcelona...\n",
      " The \"See More\" button was clicked 35 times\n",
      "Scraping hotels for Barcelona...\n",
      " The \"See More\" button was clicked 37 times\n",
      "Scraping hotels for Madrid...\n",
      " Hotel data saved to hotels_barcelona_MWC.csv\n",
      " Closing browser for Barcelona\n",
      " Hotel data saved to hotels_madrid_after_MWC.csv\n",
      " Closing browser for Madrid\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor  \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import re\n",
    "#  Function to Start Firefox Browser\n",
    "def start_browser(link, dfolder, geko_path, window_size='1500,1080'):\n",
    "    os.makedirs(dfolder, exist_ok=True)\n",
    "    options = Options()\n",
    "    options.add_argument('--start-maximized')\n",
    "    options.set_preference('privacy.trackingprotection.enabled', True)\n",
    "    service = Service(geko_path)\n",
    "    browser = webdriver.Firefox(service=service, options=options)\n",
    "    \n",
    "    width, height = map(int, window_size.split(','))\n",
    "    browser.set_window_size(width, height)\n",
    "    \n",
    "    browser.get(link)\n",
    "    time.sleep(3)  # Let page load\n",
    "    \n",
    "    return browser\n",
    "\n",
    "# Function to Search, Scroll & Scrape Hotels\n",
    "def search_and_scrape(browser, city, year, month, month_text, arrival_day, departure_day, filename):\n",
    "    try:\n",
    "        print(f\"Searching for {city} ({year}-{month}-{arrival_day} to {year}-{month}-{departure_day})\")\n",
    "\n",
    "        # Click search bar and enter city\n",
    "        WebDriverWait(browser, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\":rh:\"]'))).click()\n",
    "        time.sleep(1)\n",
    "        search_box = WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\":rh:\"]')))\n",
    "        search_box.send_keys(city)\n",
    "        time.sleep(1)\n",
    "        browser.find_element(By.XPATH, '//*[@class=\"ebbedaf8ac ab26a5d2bd e33c97ff6b\"]').click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Select correct month and year\n",
    "        target_date = month_text + ' ' + year\n",
    "        while browser.find_element(By.XPATH, '//h3[contains(@class, \"e1eebb6a1e ee7ec6b631\")]').text.strip() != target_date:\n",
    "            browser.find_element(By.XPATH, '//*[@class=\"a83ed08757 c21c56c305 f38b6daa18 d691166b09 f671049264 f4552b6561 dc72a8413c f073249358\"]').click()\n",
    "            time.sleep(1)\n",
    "\n",
    "        # Select arrival and departure dates\n",
    "        dates = browser.find_elements(By.XPATH, '//table[@class=\"eb03f3f27f\"]//td[@class=\"b80d5adb18\"]//span[@class=\"cf06f772fa ef091eb985\"]')\n",
    "        for date in dates:\n",
    "            if date.get_attribute(\"data-date\") == f\"{year}-{month}-{arrival_day}\":\n",
    "                date.click()\n",
    "            if date.get_attribute(\"data-date\") == f\"{year}-{month}-{departure_day}\":\n",
    "                date.click()\n",
    "                break\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Click search button\n",
    "        browser.find_element(By.XPATH, '//button[@class=\"a83ed08757 c21c56c305 a4c1805887 f671049264 a2abacf76b c082d89982 cceeb8986b b9fd3c6b3c\"]').click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        #Scroll and load all results\n",
    "        print(f\"Scrolling and clicking 'See More' for {city}...\")\n",
    "        i = 0\n",
    "        while True:\n",
    "            try:\n",
    "                total_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "                browser.execute_script(f\"window.scrollTo(0, {total_height});\")\n",
    "                time.sleep(2)  # Allow content to load\n",
    "\n",
    "                load_more_button = WebDriverWait(browser, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//button[@class=\"a83ed08757 c21c56c305 bf0537ecb5 f671049264 af7297d90d c0e0affd09\"]'))\n",
    "                )\n",
    "                load_more_button.click()\n",
    "                i += 1\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                break\n",
    "        \n",
    "        print(f' The \"See More\" button was clicked {i} times')\n",
    "\n",
    "        # Scrape hotels\n",
    "        print(f\"Scraping hotels for {city}...\")\n",
    "        hotel_list_data = scrape_hotel_list_page_from_selenium(browser)\n",
    "\n",
    "        # **Extract descriptions**\n",
    "        descriptions = []\n",
    "        for hotel in hotel_list_data:\n",
    "            if hotel[3] != \"NA\":\n",
    "                descriptions.append(scrape_hotel_detail_page(hotel[3]))  # Append description\n",
    "            else:\n",
    "                descriptions.append(\"NA\")  # No link, add \"NA\" to description\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        if hotel_list_data:\n",
    "            df = pd.DataFrame(hotel_list_data, columns=['Name', 'Price', 'Rating', 'Detail Link','stars','center', 'num_coments'])\n",
    "            df['Description'] = descriptions  # Add descriptions column\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\" Hotel data saved to {filename}\")\n",
    "        else:\n",
    "            print(\" No hotels found.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error during search and scrape: {e}\")\n",
    "\n",
    "    finally:\n",
    "        print(f\" Closing browser for {city}\")\n",
    "        browser.quit()\n",
    "\n",
    "# Function to Scrape Hotel List\n",
    "def scrape_hotel_list_page_from_selenium(browser):\n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    \n",
    "    hotel_data = []\n",
    "    hotels = soup.find_all('div', {'data-testid': 'property-card'})\n",
    "    \n",
    "    # for hotel in hotels:\n",
    "    #     name = hotel.find('div', {'data-testid': 'title'}).get_text(strip=True) if hotel.find('div', {'data-testid': 'title'}) else \"NA\"\n",
    "    #     price = hotel.find('span', {'class': 'f6431b446c fbfd7c1165 e84eb96b1f'}).get_text(strip=True) if hotel.find('span', {'class': 'f6431b446c fbfd7c1165 e84eb96b1f'}) else \"NA\"\n",
    "    #     rating_container = hotel.find('div', {'data-testid': 'review-score'})\n",
    "    #     rating = rating_container.find('div').get_text(strip=True) if rating_container else \"NA\"\n",
    "    #     detail_link = hotel.find('a', {'data-testid': 'title-link'})['href'] if hotel.find('a', {'data-testid': 'title-link'}) else None\n",
    "    #     detail_link = detail_link if detail_link else \"NA\"\n",
    "    #     stars = hotel.find('div', {'class': 'b3f3c831be'}).get('aria-label', 'NA').split()[0] if hotel.find('div', {'class': 'b3f3c831be'}) else \"NA\"\n",
    "\n",
    "    #     center_element = hotel.select_one('span[data-testid=\"distance\"]')\n",
    "    #     center = center_element.get_text(strip=True).replace(\"a \", \"\") if center_element else \"NA\"\n",
    "\n",
    "       \n",
    "        \n",
    "    #     hotel_data.append([name, price, rating, detail_link,stars,center])\n",
    "\n",
    "    for hotel in hotels:\n",
    "        # Name extraction\n",
    "        name_elem = hotel.find('div', {'data-testid': 'title'})\n",
    "        name = name_elem.get_text(strip=True) if name_elem else \"NA\"\n",
    "        \n",
    "        # Price extraction\n",
    "        price_elem = hotel.find('span', {'class': 'f6431b446c fbfd7c1165 e84eb96b1f'})\n",
    "        price = price_elem.get_text(strip=True) if price_elem else \"NA\"\n",
    "        \n",
    "        # Rating extraction\n",
    "        rating_container = hotel.find('div', {'data-testid': 'review-score'})\n",
    "        rating = rating_container.find('div').get_text(strip=True) if rating_container else \"NA\"\n",
    "        \n",
    "        # Detail link extraction\n",
    "        title_link = hotel.find('a', {'data-testid': 'title-link'})\n",
    "        detail_link = title_link['href'] if title_link and title_link.has_attr('href') else \"NA\"\n",
    "        \n",
    "        # Stars extraction\n",
    "        stars_elem = hotel.find('div', {'class': 'b3f3c831be'})\n",
    "        stars = stars_elem.get('aria-label', 'NA').split()[0] if stars_elem else \"NA\"\n",
    "        \n",
    "        # Center (distance) extraction: try CSS selector first, then fallback\n",
    "        center_element = hotel.select_one('span[data-testid=\"distance\"]')\n",
    "        if not center_element:\n",
    "            center_element = hotel.find(lambda tag: tag.name == \"span\" and \"km\" in tag.get_text() and \"centro\" in tag.get_text())\n",
    "        center = center_element.get_text(strip=True).replace(\"a \", \"\") if center_element else \"NA\"\n",
    "        \n",
    "        # Number of comments extraction: look for a <div> containing \"comentarios\"\n",
    "        comments_div = hotel.find(lambda tag: tag.name == \"div\" and \"comentarios\" in tag.get_text())\n",
    "        if comments_div:\n",
    "            comments_text = comments_div.get_text(strip=True)\n",
    "            match = re.search(r'(\\d+\\.?\\d*) comentario?s', comments_text)\n",
    "            num_comments = match.group(1) if match else \"NA\"\n",
    "        else:\n",
    "            num_comments = \"NA\"\n",
    "        \n",
    "        hotel_data.append([name, price, rating, detail_link, stars, center, num_comments])\n",
    "    \n",
    "    \n",
    "    return hotel_data\n",
    "\n",
    "#  Function to Scrape Hotel Description\n",
    "def scrape_hotel_detail_page(detail_url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(detail_url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            return \"Failed to load page\"\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # **Try multiple possible description locations**\n",
    "        description = soup.find('p', {'data-testid': 'property-description'})\n",
    "        if not description:\n",
    "            description = soup.find('div', {'class': 'a53cbfa6de b3efd73f69'})\n",
    "        if not description:\n",
    "            description = soup.find('span', {'class': 'e84eb96b1f'})\n",
    "        \n",
    "        return description.get_text(strip=True) if description else \"NA\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping details from {detail_url}: {e}\")\n",
    "        return \"Error\"\n",
    "def scrape_with_browser(browser_id):\n",
    "    dfolder = './downloads'\n",
    "    geko_path = ''  # Add the correct GeckoDriver path\n",
    "    link = 'https://www.booking.com/index.es.html'\n",
    "    \n",
    "    browser = start_browser(link, dfolder, geko_path)\n",
    "\n",
    "    locations = [\n",
    "        (\"Barcelona\", \"2025\", \"03\", \"marzo\", \"01\", \"07\", \"hotels_barcelona_MWC.csv\"),\n",
    "        (\"Madrid\", \"2025\", \"03\", \"marzo\", \"01\", \"07\", \"hotels_madrid_MWC.csv\"),\n",
    "        (\"Barcelona\", \"2025\", \"03\", \"marzo\", \"22\", \"28\", \"hotels_barcelona_after_MWC.csv\"),\n",
    "        (\"Madrid\", \"2025\", \"03\", \"marzo\", \"22\", \"28\", \"hotels_madrid_after_MWC.csv\")\n",
    "    ]\n",
    "\n",
    "    city, year, month, month_text, arrival_day, departure_day, filename = locations[browser_id]\n",
    "    search_and_scrape(browser, city, year, month, month_text, arrival_day, departure_day, filename)\n",
    "if __name__ == \"__main__\":\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(scrape_with_browser, range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = hotel.find('div', {'class': 'b3f3c831be'}).get('aria-label', 'NA').split()[0] if hotel.find('div', {'class': 'b3f3c831be'}) else \"NA\"\n",
    "center = hotel.find('span', {'data-testid': 'distance'}).get_text(strip=True).replace(\"a \", \"\") if hotel.find('span', {'data-testid': 'distance'}) else \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/qz/x2f42smx665c9vffjv5s55hr0000gn/T/ipykernel_52585/1812490542.py\", line 11, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/pandas/__init__.py\", line 26, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/pandas/compat/__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/pandas/compat/pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/qz/x2f42smx665c9vffjv5s55hr0000gn/T/ipykernel_52585/1812490542.py\", line 11, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/pandas/core/api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/pandas/core/dtypes/dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/DS_enviroment/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for Madrid (2025-03-01 to 2025-03-07)\n",
      "Searching for Barcelona (2025-03-01 to 2025-03-07)\n",
      "Searching for Barcelona (2025-03-22 to 2025-03-28)\n",
      "Searching for Madrid (2025-03-22 to 2025-03-28)\n",
      "Scrolling and clicking 'See More' for Madrid...\n",
      "Scrolling and clicking 'See More' for Barcelona...\n",
      "Scrolling and clicking 'See More' for Barcelona...\n",
      "Scrolling and clicking 'See More' for Madrid...\n",
      "The \"See More\" button was clicked 24 times\n",
      "Scraping hotels for Madrid...\n",
      "The \"See More\" button was clicked 35 times\n",
      "Scraping hotels for Barcelona...\n",
      "The \"See More\" button was clicked 37 times\n",
      "Scraping hotels for Madrid...\n",
      "The \"See More\" button was clicked 37 times\n",
      "Scraping hotels for Barcelona...\n",
      "Hotel data saved to hotels_madrid_MWC.csv\n",
      "Closing browser for Madrid\n",
      "Hotel data saved to hotels_barcelona_MWC.csv\n",
      "Closing browser for Barcelona\n",
      "Hotel data saved to hotels_madrid_after_MWC.csv\n",
      "Closing browser for Madrid\n",
      "Hotel data saved to hotels_barcelona_after_MWC.csv\n",
      "Closing browser for Barcelona\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor  \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to Start Firefox Browser\n",
    "def start_browser(link, dfolder, geko_path, window_size='1500,1080'):\n",
    "    os.makedirs(dfolder, exist_ok=True)\n",
    "    options = Options()\n",
    "    options.add_argument('--start-maximized')\n",
    "    options.set_preference('privacy.trackingprotection.enabled', True)\n",
    "    service = Service(geko_path)\n",
    "    browser = webdriver.Firefox(service=service, options=options)\n",
    "    \n",
    "    width, height = map(int, window_size.split(','))\n",
    "    browser.set_window_size(width, height)\n",
    "    \n",
    "    browser.get(link)\n",
    "    time.sleep(3)  # Let page load\n",
    "    return browser\n",
    "\n",
    "# Function to Search, Scroll & Scrape Hotels\n",
    "def search_and_scrape(browser, city, year, month, month_text, arrival_day, departure_day, filename):\n",
    "    try:\n",
    "        print(f\"Searching for {city} ({year}-{month}-{arrival_day} to {year}-{month}-{departure_day})\")\n",
    "\n",
    "        # Click search bar and enter city\n",
    "        WebDriverWait(browser, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\":rh:\"]'))).click()\n",
    "        time.sleep(1)\n",
    "        search_box = WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\":rh:\"]')))\n",
    "        search_box.send_keys(city)\n",
    "        time.sleep(1)\n",
    "        browser.find_element(By.XPATH, '//*[@class=\"ebbedaf8ac ab26a5d2bd e33c97ff6b\"]').click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Select correct month and year\n",
    "        target_date = month_text + ' ' + year\n",
    "        while browser.find_element(By.XPATH, '//h3[contains(@class, \"e1eebb6a1e ee7ec6b631\")]').text.strip() != target_date:\n",
    "            browser.find_element(By.XPATH, '//*[@class=\"a83ed08757 c21c56c305 f38b6daa18 d691166b09 f671049264 f4552b6561 dc72a8413c f073249358\"]').click()\n",
    "            time.sleep(1)\n",
    "\n",
    "        # Select arrival and departure dates\n",
    "        dates = browser.find_elements(By.XPATH, '//table[@class=\"eb03f3f27f\"]//td[@class=\"b80d5adb18\"]//span[@class=\"cf06f772fa ef091eb985\"]')\n",
    "        for date in dates:\n",
    "            if date.get_attribute(\"data-date\") == f\"{year}-{month}-{arrival_day}\":\n",
    "                date.click()\n",
    "            if date.get_attribute(\"data-date\") == f\"{year}-{month}-{departure_day}\":\n",
    "                date.click()\n",
    "                break\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Click search button\n",
    "        browser.find_element(By.XPATH, '//button[@class=\"a83ed08757 c21c56c305 a4c1805887 f671049264 a2abacf76b c082d89982 cceeb8986b b9fd3c6b3c\"]').click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Scroll and load all results\n",
    "        print(f\"Scrolling and clicking 'See More' for {city}...\")\n",
    "        i = 0\n",
    "        while True:\n",
    "            try:\n",
    "                total_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "                browser.execute_script(f\"window.scrollTo(0, {total_height});\")\n",
    "                time.sleep(2)  # Allow content to load\n",
    "\n",
    "                load_more_button = WebDriverWait(browser, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//button[@class=\"a83ed08757 c21c56c305 bf0537ecb5 f671049264 af7297d90d c0e0affd09\"]'))\n",
    "                )\n",
    "                load_more_button.click()\n",
    "                i += 1\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                break\n",
    "        \n",
    "        print(f'The \"See More\" button was clicked {i} times')\n",
    "\n",
    "        # Scrape hotels\n",
    "        print(f\"Scraping hotels for {city}...\")\n",
    "        hotel_list_data = scrape_hotel_list_page_from_selenium(browser)\n",
    "\n",
    "        # Extract descriptions from detail pages\n",
    "        descriptions = []\n",
    "        for hotel in hotel_list_data:\n",
    "            if hotel[3] != \"NA\":\n",
    "                descriptions.append(scrape_hotel_detail_page(hotel[3]))\n",
    "            else:\n",
    "                descriptions.append(\"NA\")\n",
    "        \n",
    "        # Build DataFrame with the updated 7 columns and add the descriptions\n",
    "        if hotel_list_data:\n",
    "            df = pd.DataFrame(hotel_list_data, columns=['Name', 'Price', 'Rating', 'Detail Link', 'Stars', 'Center', 'Num_Comments'])\n",
    "            df['Description'] = descriptions\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"Hotel data saved to {filename}\")\n",
    "        else:\n",
    "            print(\"No hotels found.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during search and scrape: {e}\")\n",
    "\n",
    "    finally:\n",
    "        print(f\"Closing browser for {city}\")\n",
    "        browser.quit()\n",
    "\n",
    "# Function to Scrape Hotel List with Center and Number of Comments\n",
    "def scrape_hotel_list_page_from_selenium(browser):\n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    hotel_data = []\n",
    "    hotels = soup.find_all('div', {'data-testid': 'property-card'})\n",
    "    \n",
    "    for hotel in hotels:\n",
    "        # Name extraction\n",
    "        name_elem = hotel.find('div', {'data-testid': 'title'})\n",
    "        name = name_elem.get_text(strip=True) if name_elem else \"NA\"\n",
    "        \n",
    "        # Price extraction\n",
    "        price_elem = hotel.find('span', {'class': 'f6431b446c fbfd7c1165 e84eb96b1f'})\n",
    "        price = price_elem.get_text(strip=True) if price_elem else \"NA\"\n",
    "        \n",
    "        # Rating extraction\n",
    "        rating_container = hotel.find('div', {'data-testid': 'review-score'})\n",
    "        rating = rating_container.find('div').get_text(strip=True) if rating_container else \"NA\"\n",
    "        \n",
    "        # Detail link extraction\n",
    "        title_link = hotel.find('a', {'data-testid': 'title-link'})\n",
    "        detail_link = title_link['href'] if title_link and title_link.has_attr('href') else \"NA\"\n",
    "        \n",
    "        # Stars extraction\n",
    "        stars_elem = hotel.find('div', {'class': 'b3f3c831be'})\n",
    "        stars = stars_elem.get('aria-label', 'NA').split()[0] if stars_elem else \"NA\"\n",
    "        \n",
    "        # Center (distance) extraction: try CSS selector first, then fallback\n",
    "        center_element = hotel.select_one('span[data-testid=\"distance\"]')\n",
    "        if not center_element:\n",
    "            center_element = hotel.find(lambda tag: tag.name == \"span\" and \"km\" in tag.get_text() and \"centro\" in tag.get_text())\n",
    "        center = center_element.get_text(strip=True).replace(\"a \", \"\") if center_element else \"NA\"\n",
    "        \n",
    "        # Number of comments extraction: look for a <div> containing \"comentarios\"\n",
    "        comments_div = hotel.find(lambda tag: tag.name == \"div\" and \"comentarios\" in tag.get_text())\n",
    "        if comments_div:\n",
    "            comments_text = comments_div.get_text(strip=True)\n",
    "            match = re.search(r'([\\d.,]+)', comments_text)\n",
    "            num_comments = match.group(1) if match else \"NA\"\n",
    "        else:\n",
    "            num_comments = \"NA\"\n",
    "        \n",
    "        hotel_data.append([name, price, rating, detail_link, stars, center, num_comments])\n",
    "    \n",
    "    return hotel_data\n",
    "\n",
    "# Function to Scrape Hotel Description from Detail Page\n",
    "def scrape_hotel_detail_page(detail_url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    try:\n",
    "        response = requests.get(detail_url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            return \"Failed to load page\"\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Try multiple possible description locations\n",
    "        description = soup.find('p', {'data-testid': 'property-description'})\n",
    "        if not description:\n",
    "            description = soup.find('div', {'class': 'a53cbfa6de b3efd73f69'})\n",
    "        if not description:\n",
    "            description = soup.find('span', {'class': 'e84eb96b1f'})\n",
    "        \n",
    "        return description.get_text(strip=True) if description else \"NA\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping details from {detail_url}: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "def scrape_with_browser(browser_id):\n",
    "    dfolder = './downloads'\n",
    "    geko_path = ''  # Add the correct GeckoDriver path\n",
    "    link = 'https://www.booking.com/index.es.html'\n",
    "    browser = start_browser(link, dfolder, geko_path)\n",
    "    locations = [\n",
    "        (\"Barcelona\", \"2025\", \"03\", \"marzo\", \"01\", \"07\", \"hotels_barcelona_MWC.csv\"),\n",
    "        (\"Madrid\", \"2025\", \"03\", \"marzo\", \"01\", \"07\", \"hotels_madrid_MWC.csv\"),\n",
    "        (\"Barcelona\", \"2025\", \"03\", \"marzo\", \"22\", \"28\", \"hotels_barcelona_after_MWC.csv\"),\n",
    "        (\"Madrid\", \"2025\", \"03\", \"marzo\", \"22\", \"28\", \"hotels_madrid_after_MWC.csv\")\n",
    "    ]\n",
    "    \n",
    "    city, year, month, month_text, arrival_day, departure_day, filename = locations[browser_id]\n",
    "    search_and_scrape(browser, city, year, month, month_text, arrival_day, departure_day, filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(scrape_with_browser, range(4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_enviroment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
