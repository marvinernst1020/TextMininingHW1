{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, StaleElementReferenceException\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from datetime import datetime\n",
    "\n",
    "# Importing libraries fto switch to the new tab\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Browser preferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffx_preferences(dfolder, download=False):\n",
    "    '''\n",
    "    Sets the preferences of the firefox browser: download path.\n",
    "    '''\n",
    "    profile = webdriver.FirefoxProfile()\n",
    "    # set download folder:\n",
    "    profile.set_preference(\"browser.download.dir\", dfolder) # you can predefine where you wanna store things in case its needed\n",
    "    profile.set_preference(\"browser.download.folderList\", 2) # 0 means to download to the desktop, 1 means to download to the default \"Downloads\" directory, 2 means to use the directory\n",
    "    profile.set_preference(\"browser.download.manager.showWhenStarting\", False) #I dont wanna see a pop up for each download so i swith it off\n",
    "    profile.set_preference(\"browser.helperApps.neverAsk.saveToDisk\",\n",
    "                           \"application/msword,application/rtf, application/csv,text/csv,image/png ,image/jpeg, application/pdf, text/html,text/plain,application/octet-stream\")\n",
    "    \n",
    "    \n",
    "\n",
    "    #POl:- Try the ublock_origin\n",
    "    profile.add_extension('/Users/polgarcia/Documents/Master/T2/Text_Mining/ublock_origin-1.62.0.xpi') #This is using the ublock-u have to download:https://addons.mozilla.org/en-US/firefox/addon/ublock-origin/#:~:text=get%20the%20extension-,Download%20file,-Extension%20Metadata\n",
    "    #profile.install_addon('/Users/polgarcia/Documents/Master/T2/Text_Mining/ublock_origin-1.62.0.xpi')\n",
    "\n",
    "\n",
    "    # this allows to download pdfs automatically\n",
    "    if download:\n",
    "        profile.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/pdf,application/x-pdf\")\n",
    "        profile.set_preference(\"pdfjs.disabled\", True) #dont want the pdf viewer to open\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument('--start-maximized')#POl: i add this to start with the browser maximied.\n",
    "    #options.add_extension('/Users/polgarcia/Documents/Master/T2/Text_Mining/ublock_origin-1.62.0.xpi') #to add the ublock_origin extension\n",
    "    options.profile = profile\n",
    "    options.set_preference('privacy.trackingprotection.enabled', True) #POL: # Enable tracking protection\n",
    "\n",
    "    return options\n",
    "\n",
    "'''Modify the start_up function to include a fixed window size'''\n",
    "def start_up(link, dfolder, geko_path,donwload=True, window_size='1500,1080'): #POL: CHANGE THIS DEPENDING ON YOUR SCREEN:\n",
    "    # geko_path='/Users/luisignaciomenendezgarcia/Dropbox/CLASSES/class_bse_text_mining/class_scraping_bse/booking/geckodriver'\n",
    "    # download_path='./downloads'\n",
    "    os.makedirs(dfolder, exist_ok=True)\n",
    "\n",
    "    options = ffx_preferences(dfolder,donwload)\n",
    "    service = Service(geko_path)\n",
    "    browser = webdriver.Firefox(service=service, options=options)\n",
    "    width, height = map(int, window_size.split(','))\n",
    "    browser.set_window_size(width, height)\n",
    "    # Enter the website address here\n",
    "    browser.get(link)\n",
    "    time.sleep(3)  # Adjust sleep time as needed\n",
    "    return browser\n",
    "\n",
    "\n",
    "def check_and_click(browser, xpath, type):\n",
    "    '''\n",
    "    Function that checks whether the object is clickable and, if so, clicks on\n",
    "    it. If not, waits one second and tries again.\n",
    "    '''\n",
    "    ck = False\n",
    "    ss = 0\n",
    "    while ck == False:\n",
    "        ck = check_obscures(browser, xpath, type)\n",
    "        time.sleep(1)\n",
    "        ss += 1\n",
    "        if ss == 15:\n",
    "            # warn_sound()\n",
    "            # return NoSuchElementException\n",
    "            ck = True\n",
    "            # browser.quit()\n",
    "\n",
    "def check_obscures(browser, xpath, type):\n",
    "    '''\n",
    "    Function that checks whether the object is being \"obscured\" by any element so\n",
    "    that it is not clickable. Important: if True, the object is going to be clicked!\n",
    "    '''\n",
    "    try:\n",
    "        if type == \"xpath\":\n",
    "            browser.find_element('xpath',xpath).click()\n",
    "        elif type == \"id\":\n",
    "            browser.find_element('id',xpath).click()\n",
    "        elif type == \"css\":\n",
    "            browser.find_element('css selector',xpath).click()\n",
    "        elif type == \"class\":\n",
    "            browser.find_element('class name',xpath).click()\n",
    "        elif type == \"link\":\n",
    "            browser.find_element('link text',xpath).click()\n",
    "    except (ElementClickInterceptedException, NoSuchElementException, StaleElementReferenceException) as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comands that i add/ were comented:\n",
    "- profile.add_extension('/Users/polgarcia/Documents/Master/T2/Text_Mining/ublock_origin-1.62.0.xpi')  \n",
    "    _why? i was trying to block  the sesion register pop-up with an add blocker, and the adds, \n",
    "    -this is not efective with the google pop-up (because it is some google shit) but it is efective with the adds\n",
    "    -you need to download the link : https://addons.mozilla.org/en-US/firefox/addon/ublock-origin/#:~:text=get%20the%20extension-,Download%20file,-Extension%20Metadata   and add it into a folder and put the path in the comand\n",
    "    - IT BLOCKS THE COOKIES \n",
    "- options.add_argument('--start-maximized')\n",
    "    -usefull in two ways, 1. When we run the code is in fullscreen, that is nice becuause we will start from the same place all.\n",
    "        2. it moves the pop up next to the table/list button, so we will be able to click on it and make the code more efficient (he said he would value that)\n",
    "- options.set_preference('privacy.trackingprotection.enabled', True) \n",
    "    - to block adds (totaly honest dont know if works, but is nice :)\n",
    "\n",
    "one of the addblockers blocs the cookies so it is nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walking through Booking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets open booking:\n",
    "\n",
    "dfolder='./downloads'\n",
    "geko_path=''\n",
    "link='https://www.booking.com/index.es.html'\n",
    "\n",
    "\n",
    "browser=start_up(dfolder=dfolder,link=link,geko_path=geko_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reject Cookies\n",
    "#POL:with the new add blockers we dont need this because it blocks the cookies :) \n",
    "\n",
    "check_and_click(browser, 'onetrust-reject-all-handler', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on the \"Where to go button\"\n",
    "browser.find_element(by=By.XPATH,value='//*[@id=\":rh:\"]').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City Selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Barcelona as the target city.\n",
    "place = 'Barcelona'\n",
    "search1 = browser.find_element(by='xpath',value='//*[@id=\":rh:\"]')\n",
    "search1.send_keys(place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on the enter date button.\n",
    "browser.find_element(by=By.XPATH,value='//*[@class=\"ebbedaf8ac ab26a5d2bd e33c97ff6b\"]').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates Selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking next until we find the right month.\n",
    "\n",
    "actual_date = datetime.now()\n",
    "actual_year = str(actual_date.year)\n",
    "\n",
    "year = '2025'\n",
    "month = 'marzo'\n",
    "month_number = '03'\n",
    "arrival_day = '01'\n",
    "departure_day = '07'\n",
    "\n",
    "# We can replace with a specific year or with the os actual year just replacing the variable year by actual_year. (Notice booking just allows you to do reservations up to one year foward.)\n",
    "target_date = month + ' ' + actual_year\n",
    "\n",
    "while browser.find_element(By.XPATH, '//h3[contains(@class, \"e1eebb6a1e ee7ec6b631\")]').text.strip() != target_date:\n",
    "    browser.find_element(By.XPATH, '//*[@class=\"a83ed08757 c21c56c305 f38b6daa18 d691166b09 f671049264 f4552b6561 dc72a8413c f073249358\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrolling down 300 Pixels.\n",
    "browser.execute_script(\"window.scrollBy(0, 300);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on the desired dates.\n",
    "path='//table[@class=\"eb03f3f27f\"]//tbody//td[@class=\"b80d5adb18\"]//span[@class=\"cf06f772fa ef091eb985\"]'\n",
    "dates = browser.find_elements('xpath',path)\n",
    "for date in dates:\n",
    "    if date.get_attribute(\"data-date\") == f\"{year}-{month_number}-{arrival_day}\":\n",
    "        date.click()\n",
    "    if date.get_attribute(\"data-date\") == f\"{year}-{month_number}-{departure_day}\":\n",
    "            date.click()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on the \"Search button\"\n",
    "browser.find_element(by=By.XPATH,value='//button[@class=\"a83ed08757 c21c56c305 a4c1805887 f671049264 a2abacf76b c082d89982 cceeb8986b b9fd3c6b3c\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POl: we dont have pop ups so no need to run\n",
    "# Close offer pop up if it appears.\n",
    "\n",
    "# Wait a few seconds to see if the pop up appears.\n",
    "time.sleep(1)\n",
    "try:\n",
    "    browser.find_element(by=By.XPATH,value='//div[@class=\"eb33ef7c47\"]//button[@class=\"a83ed08757 c21c56c305 f38b6daa18 d691166b09 ab98298258 f4552b6561\"]').click()\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading all the hotels available:\n",
    "\n",
    "I go for a duplicate block here because sometimes the first scroll fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POL:we can delete this part ( is almost the same time : 20 seconds less and it clicks the same number of times ....)\n",
    "#i will add the button of \"Tabla\"\n",
    "#Put the page into TABLE format, so we have more apartments within a page\n",
    "tablepath='//label[@for=\":r7g:-grid\" and @class=\"a53cbfa6de a454023a93\"]'\n",
    "browser.find_element(By.XPATH,tablepath).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "# Scrolling based on the height of the page.\n",
    "while True:\n",
    "    try:\n",
    "        total_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        scroll_position = (total_height*10) - 200\n",
    "        browser.execute_script(f\"window.scrollTo(0, {scroll_position});\")\n",
    "        \n",
    "        # Small wait to let the page load.\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Clicking on the load more button.\n",
    "        browser.find_element(By.XPATH, '//button[@class=\"a83ed08757 c21c56c305 bf0537ecb5 f671049264 af7297d90d c0e0affd09\"]').click()\n",
    "        i += 1\n",
    "    except NoSuchElementException:\n",
    "        break\n",
    "\n",
    "print(f'The load more button has been clicked {i} times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POL: I think this code it takes longer to run than the one that we had, (almost 1m50s minutes)\n",
    "i=0\n",
    "# Scrolling based on the height of the page.\n",
    "while True:\n",
    "    try:\n",
    "        total_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        scroll_position = (total_height*10) - 200\n",
    "        browser.execute_script(f\"window.scrollTo(0, {scroll_position});\")\n",
    "        \n",
    "        # Small wait to let the page load.\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Clicking on the load more button.\n",
    "        browser.find_element(By.XPATH, '//button[@class=\"a83ed08757 c21c56c305 bf0537ecb5 f671049264 af7297d90d c0e0affd09\"]').click()\n",
    "        i += 1\n",
    "    except NoSuchElementException:\n",
    "        break\n",
    "\n",
    "print(f'The load more button has been clicked {i} times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction with request and beautifull soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install PyPDF2\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Get the current URL from Selenium\n",
    "current_url = browser.current_url\n",
    "print(f\"Current URL: {current_url}\")\n",
    "\n",
    "response = requests.get(current_url)\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched the page with requests!\")\n",
    "\n",
    "    # Parse the page with BeautifulSoup \n",
    "    #soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    #print(soup.prettify())\n",
    "else:\n",
    "    print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_info(hotel_url):\n",
    "#     try:\n",
    "#         headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "#         response=requests.get(hotel_url,headers=headers,timeout=30)\n",
    "#         if response.status_code ==200:\n",
    "#             info=response.content\n",
    "#             return(info)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in link: {hotel_url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import requests\n",
    "# import time\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # Base URL of the page containing hotel listings\n",
    "# current_url = browser.current_url\n",
    "# print(f\"Current URL: {current_url}\")\n",
    "# base_url = current_url  \n",
    "\n",
    "# # Headers for HTTP requests\n",
    "# headers = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "# }\n",
    "\n",
    "# # Function to scrape individual hotel details from the listing page\n",
    "# def scrape_hotel_list_page(url):\n",
    "#     response = requests.get(url, headers=headers)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "#     hotel_data = []\n",
    "    \n",
    "#     # Find all hotel containers (update the selector to match your HTML structure)\n",
    "#     hotels = soup.find_all('div', {'data-testid': 'property-card'})\n",
    "    \n",
    "#     for hotel in hotels:\n",
    "#         # Extract hotel name\n",
    "#         name = hotel.find('div', {'data-testid': 'title'}).get_text(strip=True) if hotel.find('div', {'data-testid': 'title'}) else \"NA\"\n",
    "        \n",
    "#         # Extract price\n",
    "#         price = hotel.find('span', {'class': 'f6431b446c fbfd7c1165 e84eb96b1f'}).get_text(strip=True) if hotel.find('span', {'class': 'f6431b446c fbfd7c1165 e84eb96b1f'}) else \"NA\"\n",
    "        \n",
    "#         # Extract rating\n",
    "#         rating_container = hotel.find('div', {'data-testid': 'review-score'})\n",
    "#         rating = rating_container.find('div').get_text(strip=True) if rating_container else \"NA\"\n",
    "        \n",
    "#         # Extract link to detailed hotel page\n",
    "#         detail_link = hotel.find('a', {'data-testid': 'title-link'})['href'] if hotel.find('a', {'data-testid': 'title-link'}) else None \n",
    "        \n",
    "#         if detail_link:\n",
    "#             # Construct the full URL for the detailed page\n",
    "#             detail_link = f\"{detail_link}\"  #POL: this not sure\n",
    "        \n",
    "#         hotel_data.append([name, price, rating, detail_link])\n",
    "    \n",
    "#     return hotel_data\n",
    "\n",
    "# # Function to scrape hotel details from the individual hotel page\n",
    "# def scrape_hotel_detail_page(detail_url):\n",
    "#     try:\n",
    "#         response = requests.get(detail_url, headers=headers)\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "#         # Extract the specific description or other required data (update selector as needed) a53cbfa6de b3efd73f69\n",
    "#         description = soup.find('p', {'data-testid': 'property-description'}).get_text(strip=True) if soup.find('p', {'data-testid': 'property-description'}) else \"NA\"\n",
    "#         #description = soup.find('div', {'data-testid': 'property-description'}).get_text(strip=True) if soup.find('div', {'data-testid': 'property-description'}) else \"NA\"   \n",
    "        \n",
    "#         return description\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error scraping details from {detail_url}: {e}\")\n",
    "#         return \"Error\"\n",
    "\n",
    "# # Main script\n",
    "# def main():\n",
    "#     # Step 1: Scrape the listing page to get hotel data\n",
    "#     hotel_list_data = scrape_hotel_list_page(base_url)\n",
    "    \n",
    "#     # Convert the hotel list data to a DataFrame\n",
    "#     df = pd.DataFrame(hotel_list_data, columns=['Name', 'Price', 'Rating', 'Detail Link'])\n",
    "    \n",
    "#     # Step 2: Scrape each hotel's detailed page for additional information\n",
    "#     descriptions = []\n",
    "#     for index, row in df.iterrows():\n",
    "#         detail_url = row['Detail Link']\n",
    "#         if detail_url:\n",
    "#             print(f\"Scraping details for: {row['Name']} from {detail_url}\")\n",
    "#             description = scrape_hotel_detail_page(detail_url)\n",
    "#             descriptions.append(description)\n",
    "#             time.sleep(2)  # Add delay to avoid getting blocked\n",
    "#         else:\n",
    "#             descriptions.append(\"NA\")\n",
    "    \n",
    "#     # Add the descriptions to the DataFrame\n",
    "#     df['Description'] = descriptions\n",
    "    \n",
    "#     # Save the DataFrame to a CSV file\n",
    "#     df.to_csv(\"hotel_data.csv\", index=False)\n",
    "#     print(\"Hotel data saved to hotel_data.csv\")\n",
    "\n",
    "# # Run the main script\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape individual hotel details from the listing page\n",
    "def scrape_hotel_list_page_from_selenium(browser):\n",
    "    # Use the current page source from the Selenium browser\n",
    "    page_source = browser.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    hotel_data = []\n",
    "    \n",
    "    # Find all hotel containers\n",
    "    hotels = soup.find_all('div', {'data-testid': 'property-card'})\n",
    "    \n",
    "    for hotel in hotels:\n",
    "        # Extract hotel name\n",
    "        name = hotel.find('div', {'data-testid': 'title'}).get_text(strip=True) if hotel.find('div', {'data-testid': 'title'}) else \"NA\"\n",
    "        \n",
    "        # Extract price\n",
    "        price = hotel.find('span', {'class': 'f6431b446c fbfd7c1165 e84eb96b1f'}).get_text(strip=True) if hotel.find('span', {'class': 'f6431b446c fbfd7c1165 e84eb96b1f'}) else \"NA\"\n",
    "        \n",
    "        # Extract rating\n",
    "        rating_container = hotel.find('div', {'data-testid': 'review-score'})\n",
    "        rating = rating_container.find('div').get_text(strip=True) if rating_container else \"NA\"\n",
    "        \n",
    "        # Extract link to detailed hotel page\n",
    "        detail_link = hotel.find('a', {'data-testid': 'title-link'})['href'] if hotel.find('a', {'data-testid': 'title-link'}) else None \n",
    "        if detail_link:\n",
    "            # Construct the full URL for the detailed page\n",
    "            detail_link = f\"{detail_link}\"\n",
    "        \n",
    "        hotel_data.append([name, price, rating, detail_link])\n",
    "    \n",
    "    return hotel_data\n",
    "\n",
    "# Function to scrape hotel details from the individual hotel page\n",
    "def scrape_hotel_detail_page(detail_url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(detail_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract the specific description or other required data\n",
    "        description = soup.find('p', {'data-testid': 'property-description'}).get_text(strip=True) if soup.find('p', {'data-testid': 'property-description'}) else \"NA\"\n",
    "        return description\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping details from {detail_url}: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "# Main script\n",
    "def main(browser):\n",
    "    # Step 1: Scrape the listing page using Selenium\n",
    "    hotel_list_data = scrape_hotel_list_page_from_selenium(browser)\n",
    "    \n",
    "    # Convert the hotel list data to a DataFrame\n",
    "    df = pd.DataFrame(hotel_list_data, columns=['Name', 'Price', 'Rating', 'Detail Link'])\n",
    "    \n",
    "    # Step 2: Scrape each hotel's detailed page for additional information\n",
    "    descriptions = []\n",
    "    for index, row in df.iterrows():\n",
    "        detail_url = row['Detail Link']\n",
    "        if detail_url:\n",
    "            print(f\"Scraping details for: {row['Name']} from {detail_url}\")\n",
    "            description = scrape_hotel_detail_page(detail_url)\n",
    "            descriptions.append(description)\n",
    "            time.sleep(2)  # Add delay to avoid getting blocked\n",
    "        else:\n",
    "            descriptions.append(\"NA\")\n",
    "    \n",
    "    # Add the descriptions to the DataFrame\n",
    "    df['Description'] = descriptions\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(\"hotel_data_MWC.csv\", index=False)\n",
    "    print(\"Hotel data saved to hotel_data.csv\")\n",
    "\n",
    "# Run the main script\n",
    "if __name__ == \"__main__\":\n",
    "    main(browser)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction for each hotel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Creating empty lists to store the data just in case we need it.\n",
    "full_names = []\n",
    "full_prices = []\n",
    "full_score = []\n",
    "full_descriptions = []\n",
    "\n",
    "# Creating the DF that is gonna store the data.\n",
    "df = pd.DataFrame(columns=['Name', 'Price', 'Score', 'Description'])\n",
    "\n",
    "# Sabing the original handler to be able to jump from one tab to another.\n",
    "original_window = browser.current_window_handle\n",
    "\n",
    "hotels_xpath = '//div[@class=\"f6431b446c a15b38c233\"]'\n",
    "\n",
    "# Finding the full hotel list.\n",
    "hotels = browser.find_elements(By.XPATH, hotels_xpath)\n",
    "\n",
    "for index in range(10):\n",
    "#'''Use this for loop instead to do the full scrapping. It is gonna take a while......... (Around 4 Hours)'''\n",
    "#for index in range(len(hotels)):\n",
    "    try:\n",
    "        # Finding the updated list on each itaration to avoid crashes since we are not working with id.\n",
    "        hotels = browser.find_elements(By.XPATH, hotels_xpath)\n",
    "\n",
    "        # Srcolling to the hotel position according to index.\n",
    "        browser.execute_script(\"arguments[0].scrollIntoView();\", hotels[index])\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Clicking on the hotel.\n",
    "        hotels[index].click()\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Switching to the new tab using the handler.\n",
    "        new_window = [window for window in browser.window_handles if window != original_window][0]\n",
    "        browser.switch_to.window(new_window)\n",
    "\n",
    "        # Getting the hotel name.\n",
    "        try:\n",
    "            names = str(browser.find_element(By.XPATH, '//div[@data-capla-component-boundary=\"b-property-web-property-page/PropertyHeaderName\"]//h2[@class=\"d2fee87262 pp-header__title\"]').text)\n",
    "            full_names.append(names)\n",
    "        except Exception as e:\n",
    "            names = None\n",
    "            full_names.append(names)\n",
    "\n",
    "        # Getting the hotel score.\n",
    "        try:    \n",
    "            score = float(browser.find_element(By.XPATH, '//div[@id=\"js--hp-gallery-scorecard\"]').get_attribute(\"data-review-score\"))\n",
    "            full_score.append(score)\n",
    "        except Exception as e:\n",
    "            score = None\n",
    "            full_score.append(score)\n",
    "\n",
    "        # Getting the hotel description.\n",
    "        try:\n",
    "                browser.execute_script(\"document.body.style.zoom='70%'\")\n",
    "                browser.execute_script(\"window.scrollBy(0, 400);\")\n",
    "                description = str(browser.find_element(By.XPATH, '//p[@data-testid=\"property-description\"]').text)\n",
    "                full_descriptions.append(description)\n",
    "        except Exception as e:\n",
    "            description = None\n",
    "            full_descriptions.append(description)\n",
    "\n",
    "        # Getting the hotel price.\n",
    "        try:\n",
    "            browser.execute_script(\"window.scrollBy(0, 400);\")\n",
    "            price = browser.find_element(By.XPATH, '//span[@class=\"prco-valign-middle-helper\"]').text\n",
    "            full_prices.append(price)\n",
    "        except Exception as e:\n",
    "            price = None\n",
    "            full_prices.append(price)\n",
    "\n",
    "        # Appending the data to the DataFrame.\n",
    "        df.loc[len(df)] = {'Name': names, 'Price': price, 'Score': score, 'Description': description}\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Closing the new tab and switching back to the original one.\n",
    "        browser.close()\n",
    "        browser.switch_to.window(original_window)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el hotel {index}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Printing the Final DataFrame.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_enviroment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
